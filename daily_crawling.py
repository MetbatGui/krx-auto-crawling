import pandas as pd
import cloudscraper
import io
import datetime
from abc import ABC, abstractmethod

# KRX API URL ì •ì˜
OTP_URL = 'https://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'
DOWNLOAD_URL = 'https://data.krx.co.kr/comm/fileDn/download_excel/download.cmd'

class Crawler(ABC):
    @abstractmethod
    def crawl(self) -> pd.DataFrame:
        pass

class DailyNetValueCrawler(Crawler):
    
    def __init__(self, market: str, investor: str, date_str: str = None):
        super().__init__()
        self.market = market.upper()
        self.investor = investor.lower()
        
        # ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
        if date_str is None:
            # í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ YYYYMMDD í¬ë§·ìœ¼ë¡œ ì„¤ì •
            self.target_date = datetime.date.today().strftime('%Y%m%d')
        else:
            self.target_date = date_str
            
        # Cloudscraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í´ë˜ìŠ¤ ë©¤ë²„ ë³€ìˆ˜ë¡œ ì €ì¥
        self.scraper = cloudscraper.create_scraper()
        
    def create_otp_params(self) -> dict:
        params = {
            'locale': 'ko_KR',
            'invstTpCd': '',
            'strtDd': self.target_date,
            'endDd': self.target_date,
            'share': '1',
            'money': '3',
            'csvxls_isNo': 'false',
            'name': 'fileDown',
            'url': 'dbms/MDC/STAT/standard/MDCSTAT02401'
        }
        
        # 1. ì‹œì¥ êµ¬ë¶„ (market_id)
        if self.market == 'KOSPI':
            params['mktId'] = 'STK'
        elif self.market == 'KOSDAQ':
            params['mktId'] = 'KSQ'
            params['segTpCd'] = 'ALL' # ì½”ìŠ¤ë‹¥ì€ segTpCd í•„ìš”
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œì¥ ID: {self.market}. KOSPI ë˜ëŠ” KOSDAQì„ ì‚¬ìš©í•˜ì„¸ìš”.")

        # 2. íˆ¬ìì êµ¬ë¶„ (invstTpCd)
        if self.investor == 'institutions':
            params['invstTpCd'] = '7050' # ê¸°ê´€
        elif self.investor == 'foreigner':
            params['invstTpCd'] = '9000' # ì™¸êµ­ì¸
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íˆ¬ìì ìœ í˜•: {self.investor}. institutions ë˜ëŠ” foreignerë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            
        return params
    
    def crawl(self) -> pd.DataFrame:
        otp_payload = self.create_otp_params()
        
        # --- 1ë‹¨ê³„: OTP ìƒì„± ìš”ì²­ ---
        otp_response = self.scraper.post(OTP_URL, data=otp_payload, verify=True)
        otp_response.raise_for_status()
        otp_code = otp_response.text

        if not otp_code or len(otp_code) < 50:
            raise ConnectionError(f"OTP acquisition failed. KRX response: {otp_code[:100]}")

        # --- 2ë‹¨ê³„: íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ë° Pandas ë¡œë“œ ---
        download_payload = {'code': otp_code}
        download_response = self.scraper.post(DOWNLOAD_URL, data=download_payload, verify=True)
        download_response.raise_for_status()

        # Load Excel format (KRX standard)
        df = pd.read_excel(io.BytesIO(download_response.content)
                           # KRX ì—‘ì…€ íŒŒì¼ì€ ì¢…ì¢… ì²« í–‰ì„ ê±´ë„ˆë›°ì–´ì•¼ í•  ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” ìƒëµ
                           ) 
        
        # --- 3ë‹¨ê³„: ë°ì´í„° ê°€ê³µ (ìˆœë§¤ìˆ˜ ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 20) ---
        
        # 1. Sort: Descending by 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'
        df_sorted = df.sort_values(by='ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜', ascending=False)
        
        # 2. Cut: Extract top 20 data points
        df_top20 = df_sorted.head(20)
        
        # 3. Select: Keep only 'ì¢…ëª©ëª…' and 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'
        # ì´ ë‹¨ê³„ì—ì„œ ë°ì´í„°í”„ë ˆì„ì„ ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        df_top20 = df_top20[['ì¢…ëª©ëª…', 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜']]
        
        return df_top20

# --- 4ê°€ì§€ ê²½ìš°ì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì‹¤í–‰ ---
if __name__ == '__main__':
    # í˜„ì¬ ë‚ ì§œ (KST ê¸°ì¤€)
    today = datetime.date.today().strftime('%Y%m%d')

    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê³ ì • ë‚ ì§œ (20251017) ë˜ëŠ” ì˜¤ëŠ˜ ë‚ ì§œ (today) ì‚¬ìš©
    target_date_for_crawling = "20251020" 

    MARKETS = ["KOSPI", "KOSDAQ"]
    INVESTORS = ["foreigner", "institutions"]
    
    # 4ê°œì˜ DataFrameì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    all_dfs = {}

    print(f"--- KRX Net Value Top 20 Crawler Start ({target_date_for_crawling}) ---")
    
    # ë°ì´í„° ìˆ˜ì§‘ ë£¨í”„
    for market in MARKETS:
        for investor in INVESTORS:
            key = f"{market}_{investor}"
            
            try:
                print(f"\n[ğŸš€ Collecting: {market} - {investor}]")
                
                scraper_instance = DailyNetValueCrawler(
                    market=market,
                    investor=investor,
                    date_str=target_date_for_crawling
                )
                
                df_result = scraper_instance.crawl()
                all_dfs[key] = df_result
                
                print(f"âœ… Success: {key} data collected ({len(df_result)} records).")
                
            except Exception as e:
                print(f"âŒ Failed: {key} crawling failed: {e}")
                all_dfs[key] = pd.DataFrame() 
    
    # --- íŒŒì¼ ì €ì¥ ë£¨í”„ ---
    print("\n\n================================================")
    print("ğŸ’¾ Saving collected data to separate Excel files...")
    print("================================================")
    
    # íŒŒì¼ëª… ë§¤í•‘ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    market_map = {"KOSPI": "ì½”ìŠ¤í”¼", "KOSDAQ": "ì½”ìŠ¤ë‹¥"}
    investor_map = {"foreigner": "ì™¸êµ­ì¸", "institutions": "ê¸°ê´€"}

    for key, df in all_dfs.items():
        if df.empty:
            print(f"âš ï¸ Skipping file saving for {key} (DataFrame is empty).")
            continue
            
        # í‚¤ ë¶„ë¦¬: ì˜ˆì‹œ) KOSPI_foreigner -> ['KOSPI', 'foreigner']
        market_en, investor_en = key.split('_')
        
        # í•œê¸€ ì´ë¦„ ë³€í™˜
        market_kr = market_map.get(market_en)
        investor_kr = investor_map.get(investor_en)
        
        # íŒŒì¼ëª… ìƒì„±: 20251017ì½”ìŠ¤í”¼ì™¸êµ­ì¸ìˆœë§¤ìˆ˜.xlsx
        filename = f"{target_date_for_crawling}{market_kr}{investor_kr}ìˆœë§¤ìˆ˜.xlsx"
        
        try:
            # Excel íŒŒì¼ë¡œ ì €ì¥ (ì¸ë±ìŠ¤ ì œì™¸)
            df.to_excel(filename, index=False)
            print(f"âœ… Saved: '{filename}'")
            
        except Exception as e:
            print(f"âŒ Error saving '{filename}': {e}")

    print("\n--- All operations complete. ---")
