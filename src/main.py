import pandas as pd
import cloudscraper
import io
import datetime
import os
import time
from dotenv import load_dotenv
from abc import ABC, abstractmethod
from typing import Dict, Any

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (Load environment variables)
load_dotenv()

# KRX API URLs (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ, .env íŒŒì¼ì— ì„¤ì • í•„ìš”)
OTP_URL = os.getenv('KRX_OTP_URL')
DOWNLOAD_URL = os.getenv('KRX_DOWNLOAD_URL')
OUTPUT_DIR = 'krx_output' # ì—‘ì…€ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬

# --- Base Crawler Definition (ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤) ---
class Crawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ìƒì†ë°›ì•„ì•¼ í•˜ëŠ” ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    @abstractmethod
    def crawl(self, **kwargs) -> pd.DataFrame:
        """í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê³  ê°€ê³µëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        pass
    
    def get_info(self) -> str:
        """í¬ë¡¤ëŸ¬ì˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return f"Crawler: {self.__class__.__name__}"


# --- DailyNetValueCrawler Implementation (ì‚¬ìš©ìê°€ ì œê³µí•œ ì½”ë“œ) ---

class DailyNetValueCrawler(Crawler):
    """KRXì˜ íˆ¬ììë³„ ì¼ë³„ ìˆœë§¤ìˆ˜ ìƒìœ„ 20 ì¢…ëª©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    
    def __init__(self):
        """
        ì´ˆê¸°í™” ì‹œì ì— í¬ë¡¤ë§ ëŒ€ìƒ ì¸ìë¥¼ ë°›ì§€ ì•Šê³ , ê³µí†µ ìì›(scraper)ë§Œ ì¤€ë¹„í•©ë‹ˆë‹¤.
        """
        super().__init__()
        # Cloudscraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Cloudflare ìš°íšŒìš©)
        self.scraper = cloudscraper.create_scraper()
        
    def create_otp_params(self, market: str, investor: str, target_date: str) -> dict:
        """KRX OTP ë°œê¸‰ì„ ìœ„í•œ ìš”ì²­ í˜ì´ë¡œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        market = market.upper()
        investor = investor.lower()
        
        params = {
            'locale': 'ko_KR',
            'invstTpCd': '',
            'strtDd': target_date,
            'endDd': target_date,
            'share': '1', # ì£¼ì‹ìˆ˜ ê¸°ì¤€
            'money': '3', # ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€
            'csvxls_isNo': 'false',
            'name': 'fileDown',
            'url': 'dbms/MDC/STAT/standard/MDCSTAT02401' # íˆ¬ììë³„ ë§¤ë§¤ì¢…í•©
        }
        
        # 1. ì‹œì¥ êµ¬ë¶„ (mktId)
        if market == 'KOSPI':
            params['mktId'] = 'STK'
        elif market == 'KOSDAQ':
            params['mktId'] = 'KSQ'
            params['segTpCd'] = 'ALL' 
        else:
            raise ValueError(f"Unsupported market ID: {market}")

        # 2. íˆ¬ìì êµ¬ë¶„ (invstTpCd)
        if investor == 'institutions':
            params['invstTpCd'] = '7050' # ê¸°ê´€
        elif investor == 'foreigner':
            params['invstTpCd'] = '9000' # ì™¸êµ­ì¸
        else:
            raise ValueError(f"Unsupported investor type: {investor}")
            
        return params
    
    def crawl(self, market: str, investor: str, date_str: str = None) -> pd.DataFrame:
        """
        OTP ë°œê¸‰ í›„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ìƒìœ„ 20ê°œ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        :param market: ì‹œì¥ êµ¬ë¶„ ('KOSPI', 'KOSDAQ')
        :param investor: íˆ¬ìì êµ¬ë¶„ ('institutions', 'foreigner')
        :param date_str: YYYYMMDD í˜•ì‹ì˜ ë‚ ì§œ. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©.
        """
        
        # 1. ë‚ ì§œ ì„¤ì • ë° íŒŒë¼ë¯¸í„° ì¤€ë¹„
        if date_str is None:
            target_date = datetime.date.today().strftime('%Y%m%d')
        else:
            target_date = date_str
            
        market = market.upper()
        investor = investor.lower()
            
        # KRX ìš”ì²­ ê°„ê²© ìœ ì§€ë¥¼ ìœ„í•´ ì§§ì€ ì§€ì—° ì‹œê°„ ì¶”ê°€
        time.sleep(1) 
        
        otp_payload = self.create_otp_params(market, investor, target_date)
        
        print(f" Â  -> Crawling {market} ({investor}) for {target_date}")
        
        # --- 2ë‹¨ê³„: OTP ìƒì„± ìš”ì²­ ---
        if not OTP_URL:
            # ì‹¤ì œ KRX APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ KRX_OTP_URL í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            raise EnvironmentError("KRX_OTP_URL is not set in environment variables.")
            
        otp_response = self.scraper.post(OTP_URL, data=otp_payload, verify=True)
        otp_response.raise_for_status() 
        otp_code = otp_response.text

        if not otp_code or len(otp_code) < 50:
            raise ConnectionError(f"OTP acquisition failed. Check date or market status. Response snippet: {otp_code[:50]}")

        # --- 3ë‹¨ê³„: íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ---
        if not DOWNLOAD_URL:
            # ì‹¤ì œ KRX APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ KRX_DOWNLOAD_URL í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            raise EnvironmentError("KRX_DOWNLOAD_URL is not set in environment variables.")
            
        download_payload = {'code': otp_code}
        download_response = self.scraper.post(DOWNLOAD_URL, data=download_payload, verify=True)
        download_response.raise_for_status()

        # Load Excel format
        df = pd.read_excel(io.BytesIO(download_response.content))
        
        # --- 4ë‹¨ê³„: ë°ì´í„° ê°€ê³µ (ìˆœë§¤ìˆ˜ ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 20) ---
        
        # ìš”ì²­ì— ë”°ë¼ ìˆœë§¤ìˆ˜ ì»¬ëŸ¼ëª…ì„ 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'ë¡œ ëª…í™•íˆ ì§€ì •
        SORT_COL_NAME = 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜' 
        sort_col = None
        
        # ì‹¤ì œ KRX íŒŒì¼ì—ì„œ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ëª… ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë° ê³µë°± ì œê±°)
        for col in df.columns:
            if str(col).strip().replace('_', '').lower() == SORT_COL_NAME.replace('_', '').lower():
                sort_col = col
                break
        
        if sort_col is None:
            # KRX íŒŒì¼ ì»¬ëŸ¼ëª…ì´ ì¼ì¹˜í•˜ì§€ ì•Šì„ ê²½ìš°, ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ ì»¬ëŸ¼ì„ ì°¾ìŠµë‹ˆë‹¤.
            NET_VALUE_KEYWORDS = ['ìˆœë§¤ìˆ˜', 'ê±°ë˜ëŒ€ê¸ˆ']
            for col in df.columns:
                 if all(keyword in str(col).lower() for keyword in NET_VALUE_KEYWORDS):
                    sort_col = col
                    print(f" Â  -> âš ï¸ ìš”ì²­í•˜ì‹  '{SORT_COL_NAME}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, '{sort_col}' ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì²´ ì •ë ¬í•©ë‹ˆë‹¤.")
                    break

            if sort_col is None:
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    sort_col = numeric_cols[-1] 
                    print(f" Â  -> âš ï¸ ìˆœë§¤ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, ë§ˆì§€ë§‰ ìˆ«ì ì»¬ëŸ¼ '{sort_col}' ê¸°ì¤€ìœ¼ë¡œ ì„ì‹œ ì •ë ¬í•©ë‹ˆë‹¤.")
                else:
                    raise ValueError("DataFrameì— ìˆœë§¤ìˆ˜ëŒ€ê¸ˆ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f" Â  -> ìˆœë§¤ìˆ˜ ì»¬ëŸ¼ '{sort_col}' ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.")


        df_sorted = df.sort_values(by=sort_col, ascending=False)
        df_top20 = df_sorted.head(20).copy() 
        
        # ìš”ì²­ì— ë”°ë¼ ì»¬ëŸ¼ ì´ë¦„ì„ 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'ë¡œ í†µì¼ (ì²œì› ì œê±°)
        df_top20 = df_top20[['ì¢…ëª©ëª…', sort_col]].rename(columns={sort_col: SORT_COL_NAME})
        
        return df_top20


# --- Data Storage Utility ---

def save_to_excel(all_results: Dict[str, pd.DataFrame], target_date: str):
    """
    ìˆ˜ì§‘ëœ 4ê°€ì§€ ë°ì´í„°ë¥¼ ê°ê° ë³„ë„ì˜ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    íŒŒì¼ ì´ë¦„ í˜•ì‹: <ë‚ ì§œ><ì‹œì¥><íˆ¬ìì>ìˆœë§¤ìˆ˜.xlsx (ì˜ˆ: 20251020ì½”ìŠ¤í”¼ì™¸êµ­ì¸ìˆœë§¤ìˆ˜.xlsx)
    """
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"\n[ğŸ’¾ Saving results to 4 separate Excel files in {OUTPUT_DIR}/]")

    # íŒŒì¼ ì´ë¦„ ë§¤í•‘ì„ ì •ì˜í•©ë‹ˆë‹¤. (key: 'KOSPI_foreigner' -> value: 'ì½”ìŠ¤í”¼ì™¸êµ­ì¸')
    NAME_MAP = {
        'KOSPI_foreigner': 'ì½”ìŠ¤í”¼ì™¸êµ­ì¸',
        'KOSPI_institutions': 'ì½”ìŠ¤í”¼ê¸°ê´€',
        'KOSDAQ_foreigner': 'ì½”ìŠ¤ë‹¥ì™¸êµ­ì¸',
        'KOSDAQ_institutions': 'ì½”ìŠ¤ë‹¥ê¸°ê´€',
    }
    
    saved_count = 0
    
    for key, df in all_results.items():
        if not df.empty:
            try:
                # 1. íŒŒì¼ ì´ë¦„ ìƒì„±: <ë‚ ì§œ><ì‹œì¥><íˆ¬ìì>ìˆœë§¤ìˆ˜.xlsx
                korean_name_part = NAME_MAP.get(key, key.replace('_', '').title())
                filename = f"{target_date}{korean_name_part}ìˆœë§¤ìˆ˜.xlsx"
                filepath = os.path.join(OUTPUT_DIR, filename)

                df_to_save = df.copy()
                if 'ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜' in df_to_save.columns:
                     # ì‰¼í‘œ í¬ë§·íŒ…ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    df_to_save['ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'] = df_to_save['ê±°ë˜ëŒ€ê¸ˆ_ìˆœë§¤ìˆ˜'].apply(lambda x: f"{x:,}")

                # 2. DataFrameì„ ë³„ë„ì˜ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (Sheet ì´ë¦„ì€ ê¸°ë³¸ê°’ 'Sheet1')
                df_to_save.to_excel(filepath, index=False)
                print(f"   -> File '{filename}' successfully saved.")
                saved_count += 1

            except Exception as e:
                print(f"âŒ Error saving file for {key}: {e}")
        else:
            print(f"   -> Task '{key}' skipped (Empty DataFrame).")

    if saved_count > 0:
        print(f"âœ… Total {saved_count} files successfully saved.")
    else:
        print(f"âš ï¸ No files were saved.")


# --- Main Execution Logic ---

def main():
    """4ê°€ì§€ ì‹œì¥/íˆ¬ìì ì¡°í•©ì— ëŒ€í•´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    # ì˜ˆì‹œ ë‚ ì§œ ì„¤ì •: KRXëŠ” ì˜ì—…ì¼ ê¸°ì¤€ìœ¼ë¡œë§Œ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œëŠ” ì‹¤ì œ ì˜ì—…ì¼ì˜ ê³¼ê±° ë‚ ì§œ(YYYYMMDD)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    TARGET_DATE = datetime.date.today().strftime('%Y%m%d') # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ
    
    CRAWL_COMBINATIONS = [
        ("KOSPI", "foreigner"),
        ("KOSPI", "institutions"),
        ("KOSDAQ", "foreigner"),
        ("KOSDAQ", "institutions"),
    ]
    
    print(f"--- Starting KRX 4-Way Daily Net Value Crawl (Target Date: {TARGET_DATE}) ---")

    try:
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë‹¨ì¼ ìƒì„±
        crawler = DailyNetValueCrawler()
        
        all_results = {}
        
        for market, investor in CRAWL_COMBINATIONS:
            key = f"{market}_{investor}"
            print(f"\n[Task: {key}]")
            
            try:
                # í¬ë¡¤ëŸ¬ì˜ crawl ë©”ì„œë“œë¥¼ ì¬ì‚¬ìš©í•˜ë©° ê° ì¡°í•©ì— ëŒ€í•œ ì¸ìë¥¼ ì „ë‹¬í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
                df = crawler.crawl(market=market, investor=investor, date_str=TARGET_DATE)
                all_results[key] = df
                
                print(f"âœ… Success: {key} - Collected {len(df)} records.")
                print(f"--- Top 5 Data for {key} ---")
                # Pandas DataFrameì˜ ìƒìœ„ 5ê°œ í•­ëª©ë§Œ ì¶œë ¥
                print(df.head().to_markdown(index=False, numalign="left", stralign="left")) 
                
            except Exception as e:
                print(f"âŒ Failed: {key} - Error: {e}")
                all_results[key] = pd.DataFrame() # ì‹¤íŒ¨ ì‹œ ë¹ˆ DataFrame ì €ì¥
                
        # í¬ë¡¤ë§ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ì €ì¥ ì‹œë„
        if any(not df.empty for df in all_results.values()):
            save_to_excel(all_results, TARGET_DATE)
        else:
            print("\nâš ï¸ No data collected successfully. Skipping Excel save.")
                
    except Exception as e:
        print(f"\nğŸš¨ Critical initialization error: {e}")
        return

    print("\n--- All Crawl Tasks Complete ---")
    
if __name__ == '__main__':
    main()
